{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29b21b3d-3aea-423d-8e6f-39056629405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipython-sql\n",
    "!pip install -q pandas mysql-connector-python\n",
    "!pip install -q mysqlclient\n",
    "!pip install -q pymysql\n",
    "!pip install -q openpyxl\n",
    "!pip install -q beautifulsoup4\n",
    "!pip install -q nltk\n",
    "!pip install -q textblob \n",
    "!pip install -q syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98878524-d68b-4a81-8173-6c021fbd3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import syllables\n",
    "import mysql.connector\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb5c7c6-129f-48fd-aa22-936470512092",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b76f32-a863-4138-a2e6-aaab17fd93e9",
   "metadata": {},
   "source": [
    "# Setup basic prerequisite Database connections and Table\n",
    "    -  Here we use `ipython-sql` and `mysql extension` to connect to database \n",
    "    -  Setup the database tables for storing and retreving data for analysis\n",
    "    -  We created a database name `scraper` with table in it `INPUTLINKS`\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d8a3a8-3a47-4634-853c-73eca699742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dcca0b-2f9d-4303-9ca9-2cc1982bf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql mysql+mysqlconnector://root:root@localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902ef45f-ce4c-4edc-88a6-45d2df62cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql CREATE DATABASE IF NOT EXISTS scraper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9b008b-d3d1-4af2-b393-8a5f8e4f738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql USE scraper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683bc54d-440e-446e-b23d-8bcdcc30a6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "0 rows affected.\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE TABLE INPUTLINKS(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    URL VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE SCRAPERAWDATA(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    TITLE VARCHAR(255),\n",
    "    CONTENT TEXT,\n",
    "    FOREIGN KEY (URL_ID) REFERENCES INPUTLINKS(URL_ID)\n",
    ");\n",
    "\n",
    "CREATE TABLE SCRAPETEXTANALYSIS(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    POSITIVE_S FLOAT,\n",
    "    NEGATIVE_S FLOAT,\n",
    "    POLARITY_S FLOAT,\n",
    "    SUBJECTIV_S FLOAT,\n",
    "    PER_COMPLEX_WORDS FLOAT,\n",
    "    COMPLEX_WORDS INT,\n",
    "    TOTAL_WORD_COUNT INT,\n",
    "    FOG_INDEX FLOAT,\n",
    "    AVG_WORD_PER_SEN FLOAT,\n",
    "    SYLLABLE_PER_WORD FLOAT,\n",
    "    AVG_WORD_LENGTH FLOAT,\n",
    "    PERSONAL_PRONOUN INT,\n",
    "    FOREIGN KEY (URL_ID) REFERENCES SCRAPERAWDATA(URL_ID)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd505c-9a20-42ec-bff6-c7d372f0261b",
   "metadata": {},
   "source": [
    "# TextAnalysis class That composite the Scraper class\n",
    "    - Here we are text-analysis libraries like `NLTK` for analyse text data and store that into Tables in Database.\n",
    "    - later we download that data from database in xlsx format\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e59fee-c5ec-440d-92b7-df5b3bbd9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalysis:\n",
    "    '''\n",
    "        Info :\n",
    "    '''\n",
    "    \n",
    "    def preprocesstext(self,text):\n",
    "        ''' \n",
    "           info:  \n",
    "                  this will tokenise the content into small sentences then apply following steps \n",
    "                      - remove html tages\n",
    "                      - remove puntuations\n",
    "                      - remove stopwords\n",
    "        '''\n",
    "        newtext = str() \n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Define a regex pattern to match HTML tags\n",
    "            html_tags_pattern = r'<[^>]+>'\n",
    "            \n",
    "            # Use re.sub to replace HTML tags with an empty string\n",
    "            sentence = re.sub(html_tags_pattern, '', sentence)\n",
    "        \n",
    "            # Define a regex pattern to match punctuation characters\n",
    "            punctuation_pattern = r'[{}]'.format(re.escape(string.punctuation))\n",
    "                \n",
    "            # Use re.sub to replace punctuation with an empty string\n",
    "            sentence = re.sub(punctuation_pattern, '', sentence)\n",
    "        \n",
    "            # remove stop words\n",
    "            sentence = ''.join([word for word in sentence if word not in stopwords.words('english')])\n",
    "\n",
    "            # Lemmatize words\n",
    "            sentence = ''.join([lemmatizer.lemmatize(word) for word in sentence])\n",
    "    \n",
    "            newtext += sentence\n",
    "    \n",
    "        return newtext\n",
    "    # Get the number of complex word counts and total word counts\n",
    "    def __count_complex_words(self,blob):\n",
    "       words = blob.words\n",
    "    \n",
    "       # Count the number of complex words (words with more than two syllables)\n",
    "       complex_word_count = sum(1 for word in words if syllables.estimate(word) > 2)\n",
    "    \n",
    "       # Total word count\n",
    "       total_word_count = len(words)\n",
    "    \n",
    "       return complex_word_count, total_word_count     \n",
    "\n",
    "    # Calculate the percentage of complex words\n",
    "    def __calculate_percentage_complex_words(self,blob):\n",
    "       complex_word_count, total_word_count = self.__count_complex_words(blob)\n",
    "      \n",
    "       percentage_complex_words = (complex_word_count / total_word_count) * 100\n",
    "\n",
    "       return percentage_complex_words, complex_word_count, total_word_count\n",
    "\n",
    "    #Calculate the number of pronouns per sentence \n",
    "    def __count_pronouns(self,blob):      \n",
    "      # Get the list of tags for each word in the text\n",
    "      tags = blob.tags\n",
    "      \n",
    "      # Define the set of personal pronoun tags\n",
    "      personal_pronouns = set(['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "      \n",
    "      # Count the number of personal pronouns in the text\n",
    "      pronoun_count = sum(1 for word, tag in tags if tag in personal_pronouns)\n",
    "      \n",
    "      return pronoun_count\n",
    "\n",
    "\n",
    "    def document_analysis(self,text):\n",
    "        '''\n",
    "      Docs : This method calculate the following metrics:\n",
    "      Positive Score: The number of positive words or sentiments expressed in the text.\n",
    "      Negative Score: The number of negative words or sentiments expressed in the text.\n",
    "      Polarity Score: The polarity score indicates the overall sentiment of the text. It can be calculated as (Positive Score - Negative Score).\n",
    "      Subjectivity Score: The subjectivity score measures the degree of subjectivity or objectivity in the text. It usually ranges from 0 to 1, where 0 represents an objective text and 1 represents a highly subjective text.\n",
    "      Avg Sentence Length: The average number of words in each sentence of the text.\n",
    "      Percentage of Complex Words: The percentage of words in the text that are considered complex, often measured based on the number of syllables per word or other linguistic complexity measures.\n",
    "      FOG Index: The FOG Index is a readability formula that estimates the years of formal education required to understand the text. It considers sentence length and the percentage of complex words.\n",
    "      Avg Number of Words per Sentence: The average number of words in each sentence of the text.\n",
    "      Complex Word Count: The total count of complex words in the text.\n",
    "      Word Count: The total number of words in the text.\n",
    "      Syllables per Word: The average number of syllables per word in the text.\n",
    "      Personal Pronouns: The count of personal pronouns (e.g., I, you, he, she, we, they) used in the text.\n",
    "      Avg Word Length: The average length of words in the text, typically measured in characters.\n",
    "    '''\n",
    "        text_analysis_details = dict()\n",
    "        # Create a TextBlob object\n",
    "        blob = TextBlob(text)\n",
    "\n",
    "        # Calculate sentiment scores\n",
    "        text_analysis_details['positive_score'] = len([sentence for sentence in blob.sentences if sentence.sentiment.polarity > 0])\n",
    "        text_analysis_details['negative_score'] = len([sentence for sentence in blob.sentences if sentence.sentiment.polarity < 0])\n",
    "        text_analysis_details['polarity_score'] = blob.sentiment.polarity\n",
    "        text_analysis_details['subjectivity_score'] = blob.sentiment.subjectivity\n",
    "     \n",
    "        # Calculate average sentence length\n",
    "        text_analysis_details['avg_sentence_length'] = sum(len(sentence.words) for sentence in blob.sentences) / len(blob.sentences)  \n",
    "      \n",
    "        percentage_complex_words, complex_word_count, total_word_count = self.__calculate_percentage_complex_words(blob)\n",
    "        text_analysis_details['percentage_complex_words'] = percentage_complex_words\n",
    "        text_analysis_details['complex_word_count'] = complex_word_count\n",
    "        text_analysis_details['total_word_count'] = total_word_count\n",
    "     \n",
    "        # Calculate the FOG Index (requires more detailed calculations)\n",
    "        # FOG Index formula: 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        text_analysis_details['fog_index'] = 0.4 * ((sum(len(sentence.words) for sentence in blob.sentences) / len(blob.sentences)) + text_analysis_details['percentage_complex_words'])  \n",
    "     \n",
    "        # Calculate average number of words per sentence\n",
    "        text_analysis_details['avg_words_per_sentence'] = len(blob.words) / len(blob.sentences) \n",
    "     \n",
    "        # Calculate syllabus per word length\n",
    "        text_analysis_details['syllable_per_word']= (sum([syllables.estimate(word) for word in blob.words])/len([syllables.estimate(word) for word in blob.words]))\n",
    "     \n",
    "        # Calculate average word length\n",
    "        text_analysis_details['avg_word_length'] = (sum(len(word) for word in blob.words) / len(blob.words)) \n",
    "\n",
    "        # Calculate personal pronoun\n",
    "        text_analysis_details['personal_pronoun'] = self.__count_pronouns(blob)\n",
    "\n",
    "        return text_analysis_details\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50247d17-487c-42c8-b6b5-e1f53d439087",
   "metadata": {},
   "source": [
    "# Scraper class to Scrape Data from InputLinks\n",
    "    - Here we are going to use `mysql extension`, `beautifulsoup4` and `multi-threading` for scraping the data\n",
    "    - later we want that data to be store in new table called `RAWDATATABLE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def0c560-dd53-43d5-8b9c-075e24de9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \n",
    "    '''\n",
    "        Info : This class help the user to scrape the data from urls and do throughly analysis on that text data\n",
    "        \n",
    "        Usage Example:\n",
    "            - Create an Instance of Scraper class (That will connect to you given database)\n",
    "            - Provide a file_name of xlsx format in which url_id and url is given (get_data_from_xlsx_to_sql to insert data from xlsx to sql)\n",
    "            - This class also have partition_raw_data (Method to make chunks so we can use it in multithreading)\n",
    "            - scrape_data_from_url This method is used to scrape data from url and store that into new table called SCRAPERAWDATA.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,TextAnalysis):\n",
    "        \n",
    "        self.connection = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"scraper\"\n",
    "        );\n",
    "        \n",
    "        self.textanalysis = TextAnalysis()\n",
    "        \n",
    "        # Create a cursor object for executing SQL queries\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def get_data_from_xlsx_to_sql(self,file_name):\n",
    "        '''\n",
    "            Info : Before running this cell make sure you have database with prerquisite database and database tables..\n",
    "            Parameter : \n",
    "                file_name (file should be in same working directory) example Input.xlsx\n",
    "        '''\n",
    "        \n",
    "        self.file_path = os.path.join(os.getcwd(),file_name)\n",
    "        self.df = pd.read_excel(self.file_path)\n",
    "        list_of_tuples = [tuple(row) for row in self.df.to_records(index=False)]\n",
    "            \n",
    "        \n",
    "        try:           \n",
    "            # Define the INSERT INTO statement\n",
    "            insert_query = \"INSERT INTO INPUTLINKS (URL_ID, URL) VALUES (%s, %s)\"\n",
    "            \n",
    "            # Loop through the list and execute the INSERT INTO statement for each tuple\n",
    "            for values in list_of_tuples:\n",
    "                self.cursor.execute(insert_query, values)\n",
    "            \n",
    "            # Commit the transaction to save the changes to the database\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()\n",
    "            print('Insertion Failed')\n",
    "        else:\n",
    "            print('Insertion Successful')\n",
    "\n",
    "    def partition_raw_data(self,chunk_size,table_name):\n",
    "        '''\n",
    "            Info : use a generator function to partition a list into smaller equal-sized sublists efficiently\n",
    "            Parameters : \n",
    "                file_name : Name of a file\n",
    "                chunk_size : divide the whole data into number of chunks\n",
    "            \n",
    "            steps:\n",
    "                1) Fetch the url from table in database\n",
    "                2) create small chunks\n",
    "\n",
    "            Returns : List of Tuples where tuple contain url_id and url to scrape data in chunks.\n",
    "\n",
    "            usage example:\n",
    "            # Create a generator\n",
    "            result_generator = partition_raw_data(chunk_size=2)\n",
    "\n",
    "            # Iterate through the generator to get the sublists\n",
    "            result_generator = scrap.partition_raw_data(57,'INPUTLINKS')\n",
    "            rawdata_first_half = next(result_generator)\n",
    "            rawdata_second_half = next(result_generator)\n",
    "        '''\n",
    "        \n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.cursor.execute(query)\n",
    "        while True:\n",
    "            rows = self.cursor.fetchmany(chunk_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "\n",
    "    def scrape_data_from_url(self,data):\n",
    "       '''\n",
    "          Prerequisites : Before you run this code make sure you have relevant database and database tables to store.\n",
    "          Info : This method is to get the url scrape the content and title and store it to database Table.\n",
    "          Parameters : \n",
    "              data (List of Tuples) : [\n",
    "                  (123,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/'),\n",
    "                  (124,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-1/'),\n",
    "                  (125,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-0/')\n",
    "              ]\n",
    "       '''\n",
    "       if not data:\n",
    "           return {'Error':'Please provide valid data type.'}\n",
    "       else:\n",
    "           if isinstance(data,list):\n",
    "               for value in data:\n",
    "                   url_id,url = value\n",
    "                   response = requests.get(url)\n",
    "\n",
    "                   # Check if the request was successful (status code 200)\n",
    "                   if response.status_code == 200:\n",
    "                       html_content = response.text\n",
    "                       soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                       title = soup.find('h1', class_='entry-title').text if soup.find('h1', class_='entry-title') is not None else soup.find('h1', class_='tdb-title-text').text if soup.find('h1', class_='tdb-title-text') is not None else None\n",
    "                       # Find the <div> with class 'td-post-content'\n",
    "                       post_content_div = soup.find('div', class_='td-post-content') if soup.find('div', class_='td-post-content') is not None else soup.find('div', class_='tdb-block-inner') if soup.find('div', class_='tdb-block-inner') is not None else None\n",
    "                        \n",
    "                       # Check if the <div> was found \n",
    "                       if post_content_div and title is not None: \n",
    "                           # if got the content and title in given url....\n",
    "                           \n",
    "                           # Find all <p> elements inside the <div> without a class name\n",
    "                           paragraphs_without_class =  post_content_div.find_all('p', class_=False)\n",
    "                           \n",
    "                           # Loop through the selected <p> elements and print their text content\n",
    "                           content = str()\n",
    "                           for paragraph in paragraphs_without_class:\n",
    "                               content += paragraph.get_text()\n",
    "                               try:\n",
    "                                   with database_lock:\n",
    "                                       query = 'INSERT INTO SCRAPERAWDATA(URL_ID,TITLE,CONTENT) VALUES(%s,%s,%s)'\n",
    "                                       values = (url_id,title,content)\n",
    "                                       self.cursor.execute(query, values)\n",
    "                                       self.connection.commit()\n",
    "                               except Exception as e:\n",
    "                                   self.connection.rollback()\n",
    "                               else:\n",
    "                                   print('URL_ID',url_id,'Process Successfully')\n",
    "                               finally:\n",
    "                                   print('Scraping and processing text is completed.')\n",
    "                       else:\n",
    "                           query = 'INSERT INTO SCRAPERAWDATA(URL_ID,TITLE,CONTENT) VALUES(%s,%s,%s)'\n",
    "                           values = (url_id,None,None)\n",
    "                           self.cursor.execute(query, values)\n",
    "                           self.connection.commit()\n",
    "                           print(f'No Content Found at {url_id}')\n",
    "                   else:\n",
    "                       return {'Error','Failed to retrieve the web page.'}\n",
    "           else:\n",
    "               return {'Error':'Please provide valid data type.'}\n",
    "\n",
    "    def store_document_analysis(self,data):\n",
    "        if not data:\n",
    "            return {'Error':'Please provide valid data type.'}\n",
    "        else:\n",
    "            if isinstance(data,list):\n",
    "                for value in data:\n",
    "                    url_id,title,content = value\n",
    "                    try:\n",
    "                        content_analysis_detail = self.textanalysis.document_analysis(content)\n",
    "                        with database_lock:\n",
    "                            query = '''INSERT INTO SCRAPERAWDATA(URL_ID,POSITIVE_S,NEGATIVE_S,POLARITY_S,\n",
    "                            SUBJECTIV_S,PER_COMPLEX_WORDS,COMPLEX_WORDS,TOTAL_WORD_COUNT,FOG_INDEX,AVG_WORD_PER_SEN,\n",
    "                            SYLLABLE_PER_WORD,AVG_WORD_LENGTH,PERSONAL_PRONOUN) \n",
    "                            VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                            '''\n",
    "                            values = (url_id,\n",
    "                                      content_analysis_detail['positive_score'],\n",
    "                                      content_analysis_detail['negative_score'],\n",
    "                                      content_analysis_detail['polarity_score'],\n",
    "                                      content_analysis_detail['subjectivity_score'],\n",
    "                                      content_analysis_detail['avg_sentence_length'],\n",
    "                                      content_analysis_detail['percentage_complex_words'],\n",
    "                                      content_analysis_detail['complex_word_count'],\n",
    "                                      content_analysis_detail['total_word_count''fog_index'],\n",
    "                                      content_analysis_detail['avg_words_per_sentence'],\n",
    "                                      content_analysis_detail['syllable_per_word'],\n",
    "                                      content_analysis_detail['avg_word_length'],\n",
    "                                      content_analysis_detail['personal_pronoun']\n",
    "                                     )     \n",
    "                            self.cursor.execute(query, values)\n",
    "                            self.connection.commit()\n",
    "                    except Exception as e:\n",
    "                        with database_lock:\n",
    "                            query = '''INSERT INTO SCRAPERAWDATA(URL_ID,POSITIVE_S,NEGATIVE_S,POLARITY_S,\n",
    "                            SUBJECTIV_S,PER_COMPLEX_WORDS,COMPLEX_WORDS,TOTAL_WORD_COUNT,FOG_INDEX,AVG_WORD_PER_SEN,\n",
    "                            SYLLABLE_PER_WORD,AVG_WORD_LENGTH,PERSONAL_PRONOUN) \n",
    "                            VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                            '''\n",
    "                            values = (url_id,None,None,None,None,None,None,None,None,None,None,None,None)\n",
    "                            self.cursor.execute(query, values)\n",
    "                            self.connection.commit()\n",
    "                    else:\n",
    "                        print('URL_ID',url_id,'Process Successfully')\n",
    "                    finally:\n",
    "                        print('text analysis is completed.')\n",
    "                    print(url_id,content_analysis_detail)\n",
    "            else:\n",
    "                return {'Error':'Please provide valid data type.'}\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f4bc753-2ce6-45ed-9443-bc3fbf62a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Failed\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraper(TextAnalysis)\n",
    "scrap.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "result_generator = scrap.partition_raw_data(57,'INPUTLINKS')\n",
    "rawdata_first_half = next(result_generator)\n",
    "rawdata_second_half = next(result_generator)\n",
    "# scrap.scrape_data_from_url(rawdata_second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543be92-c590-41b7-a6f5-be318015bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-68 (scrape_data_from_url):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 633, in cmd_query\n",
      "    self._cmysql.query(\n",
      "_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_13660\\3406682376.py\", line 128, in scrape_data_from_url\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py\", line 330, in execute\n",
      "    result = self._cnx.cmd_query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py\", line 77, in wrapper\n",
      "    return method(cnx, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 641, in cmd_query\n",
      "    raise get_mysql_exception(\n",
      "mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_13660\\3406682376.py\", line 131, in scrape_data_from_url\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 517, in rollback\n",
      "    self._cmysql.rollback()\n",
      "_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and processing text is completed.\n",
      "Scraping and processing text is completed.\n",
      "Scraping and processing text is completed.\n",
      "Scraping and processing text is completed.\n"
     ]
    }
   ],
   "source": [
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_second_half,))\n",
    "        \n",
    "# Start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "        \n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c4e18-1455-40e7-a554-8e7893cc108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Failed\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraper(TextAnalysis)\n",
    "scrap.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "result_generator = scrap.partition_raw_data(57,'SCRAPERAWDATA')\n",
    "rawdata_first_half = next(result_generator)\n",
    "rawdata_second_half = next(result_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357336e5-1769-4297-a868-7a0c5ac5d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (store_document_analysis):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 170, in store_document_analysis\n",
      "Exception in thread Thread-7 (store_document_analysis):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 170, in store_document_analysis\n",
      "KeyError: 'total_word_countfog_index'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 633, in cmd_query\n",
      "KeyError: 'total_word_countfog_index'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 633, in cmd_query\n",
      "    self._cmysql.query(\n",
      "_mysql_connector.MySQLInterfaceError: Unknown column 'POSITIVE_S' in 'field list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self._cmysql.query(\n",
      "_mysql_connector.MySQLInterfaceError: Unknown column 'POSITIVE_S' in 'field list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 186, in store_document_analysis\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 186, in store_document_analysis\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py\", line 330, in execute\n",
      "    result = self._cnx.cmd_query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py\", line 77, in wrapper\n",
      "    return method(cnx, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 641, in cmd_query\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py\", line 330, in execute\n",
      "    raise get_mysql_exception(\n",
      "mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'POSITIVE_S' in 'field list'\n",
      "    result = self._cnx.cmd_query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py\", line 77, in wrapper\n",
      "    return method(cnx, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 641, in cmd_query\n",
      "    raise get_mysql_exception(\n",
      "mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'POSITIVE_S' in 'field list'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text analysis is completed.\n",
      "text analysis is completed.\n"
     ]
    }
   ],
   "source": [
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.store_document_analysis, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.store_document_analysis, args=(rawdata_second_half,))\n",
    "        \n",
    "# Start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "        \n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
