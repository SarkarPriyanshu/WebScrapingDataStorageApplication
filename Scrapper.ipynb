{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221da3b2-8118-4fb9-b1c0-96c18433b895",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. <span style=\"color: #3498db;\">Prerequisites and Setting Environment</span>\n",
    "2. <span style=\"color: #3498db;\">Installing Relevant Libraries and Importing Them</span>\n",
    "3. <span style=\"color: #3498db;\">Setup Basic Prerequisite: Database Connections and Tables</span>\n",
    "4. <span style=\"color: #3498db;\">TextAnalysis Class: Composition with Scraper Class</span>\n",
    "5. <span style=\"color: #3498db;\">Scraper Class: Data Scraping</span>\n",
    "6. <span style=\"color: #3498db;\">Code Explanation: Scraping and Data Processing Workflow</span>\n",
    "7. <span style=\"color: #3498db;\">Exporting the Text Analysis in Excel Format</span>\n",
    "8. <span style=\"color: #3498db;\">How Anyone Can Apply This Project as a Base Blueprint for Scraping and Storing in MySQL</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94522c-6158-47b5-a62a-e3625a404205",
   "metadata": {},
   "source": [
    "## 1. Installing Relevant Libraries and Importing Them\n",
    "\n",
    "In this section, we discuss the necessary libraries and modules required for our project. We'll also demonstrate how to install and import these libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b21b3d-3aea-423d-8e6f-39056629405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipython-sql\n",
    "!pip install -q pandas mysql-connector-python\n",
    "!pip install -q mysqlclient\n",
    "!pip install -q pymysql\n",
    "!pip install -q openpyxl\n",
    "!pip install -q beautifulsoup4\n",
    "!pip install -q nltk\n",
    "!pip install -q textblob \n",
    "!pip install -q syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98878524-d68b-4a81-8173-6c021fbd3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import syllables\n",
    "import mysql.connector\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb5c7c6-129f-48fd-aa22-936470512092",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b76f32-a863-4138-a2e6-aaab17fd93e9",
   "metadata": {},
   "source": [
    "## 2. Setup Basic Prerequisite: Database Connections and Tables\r\n",
    "\r\n",
    "- In this section, we utilize `ipython-sql` and the `mysql extension` to establish database connections.\r\n",
    "\r\n",
    "- We set up database tables to facilitate data storage and retrieval for analysis purposes.\r\n",
    "\r\n",
    "- The following database tables have been created:\r\n",
    "  - INPUTLINKS\r\n",
    "  - SCRAPERAWDATA\r\n",
    "  - SCRAPETEXTANALYSIS\r\n",
    "\r\n",
    "- These tables are designed to enable seamless data storage, retrieval, and updates as needed.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d8a3a8-3a47-4634-853c-73eca699742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dcca0b-2f9d-4303-9ca9-2cc1982bf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql mysql+mysqlconnector://root:root@localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902ef45f-ce4c-4edc-88a6-45d2df62cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql CREATE DATABASE IF NOT EXISTS scraper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9b008b-d3d1-4af2-b393-8a5f8e4f738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql USE scraper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683bc54d-440e-446e-b23d-8bcdcc30a6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "0 rows affected.\n",
      "0 rows affected.\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE TABLE INPUTLINKS(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    URL VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE SCRAPERAWDATA(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    TITLE VARCHAR(255),\n",
    "    CONTENT TEXT,\n",
    "    FOREIGN KEY (URL_ID) REFERENCES INPUTLINKS(URL_ID)\n",
    ");\n",
    "\n",
    "CREATE TABLE SCRAPETEXTANALYSIS(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    POSITIVE_S FLOAT,\n",
    "    NEGATIVE_S FLOAT,\n",
    "    POLARITY_S FLOAT,\n",
    "    SUBJECTIV_S FLOAT,\n",
    "    PER_COMPLEX_WORDS FLOAT,\n",
    "    COMPLEX_WORDS INT,\n",
    "    TOTAL_WORD_COUNT INT,\n",
    "    FOG_INDEX FLOAT,\n",
    "    AVG_WORD_PER_SEN FLOAT,\n",
    "    SYLLABLE_PER_WORD FLOAT,\n",
    "    AVG_WORD_LENGTH FLOAT,\n",
    "    PERSONAL_PRONOUN INT,\n",
    "    FOREIGN KEY (URL_ID) REFERENCES SCRAPERAWDATA(URL_ID)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd505c-9a20-42ec-bff6-c7d372f0261b",
   "metadata": {},
   "source": [
    "## 3. TextAnalysis Class: Composition with Scraper Class\r\n",
    "\r\n",
    "- The `TextAnalysis` class leverages text analysis libraries such as `NLTK` to analyze text data and store the results in database tables.\r\n",
    "\r\n",
    "- This class serves the purpose of separating the text analysis process, promoting modular design.\r\n",
    "\r\n",
    "- It establishes a composition relationship with the `Scraper` class, allowing for the integration of text analysis capabilities within the broader scraping and data processing context.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88e59fee-c5ec-440d-92b7-df5b3bbd9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalysis:\n",
    "    \"\"\"\n",
    "    The TextAnalysis class provides methods for text preprocessing and analysis.\n",
    "\n",
    "    This class includes methods to preprocess text by removing HTML tags, punctuation,\n",
    "    and stopwords, as well as performing text analysis and calculating various\n",
    "    metrics such as sentiment scores, readability, and word statistics.\n",
    "\n",
    "    Attributes:\n",
    "        None\n",
    "\n",
    "    Methods:\n",
    "        preprocesstext(text):\n",
    "            Preprocess the input text by tokenizing, removing HTML tags, punctuation,\n",
    "            and stopwords, and lemmatizing words.\n",
    "\n",
    "        document_analysis(text):\n",
    "            Calculate various text analysis metrics, including sentiment scores,\n",
    "            subjectivity, average sentence length, percentage of complex words, FOG Index,\n",
    "            average number of words per sentence, complex word count, total word count,\n",
    "            syllables per word, average word length, and personal pronoun count.\n",
    "\n",
    "    Example Usage:\n",
    "        analyzer = TextAnalysis()\n",
    "        preprocessed_text = analyzer.preprocesstext(raw_text)\n",
    "        analysis_results = analyzer.document_analysis(analyzed_text)\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocesstext(self,text):\n",
    "        \"\"\"\n",
    "        Preprocess the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text after removing HTML tags, punctuation,\n",
    "                 stopwords, and lemmatization.\n",
    "        \"\"\"\n",
    "        newtext = str() \n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Define a regex pattern to match HTML tags\n",
    "            html_tags_pattern = r'<[^>]+>'\n",
    "            \n",
    "            # Use re.sub to replace HTML tags with an empty string\n",
    "            sentence = re.sub(html_tags_pattern, '', sentence)\n",
    "        \n",
    "            # Define a regex pattern to match punctuation characters\n",
    "            punctuation_pattern = r'[{}]'.format(re.escape(string.punctuation))\n",
    "                \n",
    "            # Use re.sub to replace punctuation with an empty string\n",
    "            sentence = re.sub(punctuation_pattern, '', sentence)\n",
    "        \n",
    "            # remove stop words\n",
    "            sentence = ''.join([word for word in sentence if word not in stopwords.words('english')])\n",
    "\n",
    "            # Lemmatize words\n",
    "            sentence = ''.join([lemmatizer.lemmatize(word) for word in sentence])\n",
    "    \n",
    "            newtext += sentence\n",
    "    \n",
    "        return newtext\n",
    "\n",
    "    \n",
    "    def __count_complex_words(self,blob):\n",
    "       words = blob.words\n",
    "    \n",
    "       # Count the number of complex words (words with more than two syllables)\n",
    "       complex_word_count = sum(1 for word in words if syllables.estimate(word) > 2)\n",
    "    \n",
    "       # Total word count\n",
    "       total_word_count = len(words)\n",
    "    \n",
    "       return complex_word_count, total_word_count     \n",
    "\n",
    "\n",
    "    def __calculate_percentage_complex_words(self,blob):\n",
    "       complex_word_count, total_word_count = self.__count_complex_words(blob)\n",
    "      \n",
    "       percentage_complex_words = (complex_word_count / total_word_count if total_word_count != 0 else 1) * 100\n",
    "\n",
    "       return percentage_complex_words, complex_word_count, total_word_count\n",
    "\n",
    "\n",
    "    def __count_pronouns(self,blob):      \n",
    "      # Get the list of tags for each word in the text\n",
    "      tags = blob.tags\n",
    "      \n",
    "      # Define the set of personal pronoun tags\n",
    "      personal_pronouns = set(['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "      \n",
    "      # Count the number of personal pronouns in the text\n",
    "      pronoun_count = sum(1 for word, tag in tags if tag in personal_pronouns)\n",
    "      \n",
    "      return pronoun_count\n",
    "\n",
    "\n",
    "    def document_analysis(self,text):\n",
    "        '''\n",
    "        Info : Analyze the input text and calculate various text analysis metrics.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing various text analysis metrics,\n",
    "                  such as sentiment scores, subjectivity, average sentence length,\n",
    "                  percentage of complex words, FOG Index, average number of words\n",
    "                  per sentence, complex word count, total word count, syllables\n",
    "                  per word, average word length, and personal pronoun count.\n",
    "                  \n",
    "              Docs : This method calculate the following metrics:\n",
    "              Positive Score: The number of positive words or sentiments expressed in the text.\n",
    "              Negative Score: The number of negative words or sentiments expressed in the text.\n",
    "              Polarity Score: The polarity score indicates the overall sentiment of the text. It can be calculated as (Positive Score - Negative Score).\n",
    "              Subjectivity Score: The subjectivity score measures the degree of subjectivity or objectivity in the text. It usually ranges from 0 to 1, where 0 represents an objective text and 1 represents a highly subjective text.\n",
    "              Avg Sentence Length: The average number of words in each sentence of the text.\n",
    "              Percentage of Complex Words: The percentage of words in the text that are considered complex, often measured based on the number of syllables per word or other linguistic complexity measures.\n",
    "              FOG Index: The FOG Index is a readability formula that estimates the years of formal education required to understand the text. It considers sentence length and the percentage of complex words.\n",
    "              Avg Number of Words per Sentence: The average number of words in each sentence of the text.\n",
    "              Complex Word Count: The total count of complex words in the text.\n",
    "              Word Count: The total number of words in the text.\n",
    "              Syllables per Word: The average number of syllables per word in the text.\n",
    "              Personal Pronouns: The count of personal pronouns (e.g., I, you, he, she, we, they) used in the text.\n",
    "              Avg Word Length: The average length of words in the text, typically measured in characters.\n",
    "    '''\n",
    "        text_analysis_details = dict()\n",
    "        # Create a TextBlob object\n",
    "        blob = TextBlob(text)\n",
    "\n",
    "        # Calculate sentiment scores\n",
    "        text_analysis_details['positive_score'] = len([sentence for sentence in blob.sentences if sentence.sentiment.polarity > 0])\n",
    "        text_analysis_details['negative_score'] = len([sentence for sentence in blob.sentences if sentence.sentiment.polarity < 0])\n",
    "        text_analysis_details['polarity_score'] = blob.sentiment.polarity\n",
    "        text_analysis_details['subjectivity_score'] = blob.sentiment.subjectivity\n",
    "     \n",
    "        # Calculate average sentence length\n",
    "        text_analysis_details['avg_sentence_length'] = sum(len(sentence.words) for sentence in blob.sentences) / len(blob.sentences) if len(blob.sentences) != 0 else 1  \n",
    "      \n",
    "        percentage_complex_words, complex_word_count, total_word_count = self.__calculate_percentage_complex_words(blob)\n",
    "        text_analysis_details['percentage_complex_words'] = percentage_complex_words\n",
    "        text_analysis_details['complex_word_count'] = complex_word_count\n",
    "        text_analysis_details['total_word_count'] = total_word_count\n",
    "     \n",
    "        # Calculate the FOG Index (requires more detailed calculations)\n",
    "        # FOG Index formula: 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        text_analysis_details['fog_index'] = 0.4 * ((sum(len(sentence.words) for sentence in blob.sentences) / len(blob.sentences) if len(blob.sentences) != 0 else 1 ) + text_analysis_details['percentage_complex_words'])  \n",
    "     \n",
    "        # Calculate average number of words per sentence\n",
    "        text_analysis_details['avg_words_per_sentence'] = len(blob.words) / len(blob.sentences) if len(blob.sentences) != 0 else len(blob.words) / 1 \n",
    "     \n",
    "        # Calculate syllabus per word length\n",
    "        text_analysis_details['syllable_per_word']= (sum([syllables.estimate(word) for word in blob.words])/len([syllables.estimate(word) for word in blob.words]) if len([syllables.estimate(word) for word in blob.words])!=0 else 1)\n",
    "     \n",
    "        # Calculate average word length\n",
    "        text_analysis_details['avg_word_length'] = (sum(len(word) for word in blob.words) / len(blob.words)) if len(blob.words) != 0 else (sum(len(word) for word in blob.words) / 1) \n",
    "\n",
    "        # Calculate personal pronoun\n",
    "        text_analysis_details['personal_pronoun'] = self.__count_pronouns(blob)\n",
    "\n",
    "        return text_analysis_details\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50247d17-487c-42c8-b6b5-e1f53d439087",
   "metadata": {},
   "source": [
    "## 4. Scraper Class: Data Scraping from InputLinks\r\n",
    "\r\n",
    "- The `Scraper` class is designed to efficiently scrape data from the `InputLinks` source using various technologies and techniques.\r\n",
    "\r\n",
    "- Key Technologies:\r\n",
    "  - **MySQL Extension:** It utilizes the `mysql extension` to manage database connections and operations.\r\n",
    "  - **Beautiful Soup 4 (beautifulsoup4):** The class leverages the `beautifulsoup4` library to parse and extract data from web pages with ease.\r\n",
    "  - **Multithreading:** To enhance data retrieval speed, the class employs multithreading, allowing for the concurrent processing of multiple web pages.\r\n",
    "\r\n",
    "- Data Storage:\r\n",
    "  - The scraped data is intended to be stored in a dedicated table named `RAWDATATABLE` within the database for further analysis and processing.\r\n",
    "\r\n",
    "- Purpose:\r\n",
    "  - The primary goal of this class is to streamline the data scraping process, ensuring efficient extraction and storage of data from the `InputLinks` source.\r\n",
    "\r\n",
    "- Note:\r\n",
    "  - Before using this class, ensure that you have set up the necessary database connections and tables to accommodate the scraped data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "def0c560-dd53-43d5-8b9c-075e24de9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \"\"\"\n",
    "        The Scraper class is responsible for web scraping, data processing, and storage.\n",
    "        \n",
    "        It connects to a MySQL database, loads data from an Excel file, partitions data\n",
    "        into smaller sublists, scrapes article data from URLs, performs text analysis, and\n",
    "        stores the results in the database.\n",
    "    \n",
    "        Args:\n",
    "            text_analysis (TextAnalysis): An instance of the TextAnalysis class used for\n",
    "                text analysis and data storage.\n",
    "    \n",
    "        Attributes:\n",
    "            connection (MySQLConnection): A MySQL database connection.\n",
    "            cursor (MySQLCursor): A MySQL cursor for executing queries.\n",
    "            text_analysis (TextAnalysis): An instance of the TextAnalysis class.\n",
    "    \n",
    "        Methods:\n",
    "            __init__(self, text_analysis): Constructor for initializing the Scraper object.\n",
    "            get_data_from_xlsx_to_sql(self, file_name): Load data from an Excel file into a\n",
    "                SQL database table.\n",
    "            partition_raw_data(self, data_list, chunk_size): Partition a list into smaller\n",
    "                equal-sized sublists efficiently.\n",
    "            scrape_data_from_url(self, url_list): Scrape the title and content of articles\n",
    "                from a list of URLs and store them in the database.\n",
    "            store_document_analysis(self, text_data): Use TextAnalysis to perform analysis\n",
    "                on text data and store the results in the database.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,TextAnalysis):\n",
    "        \"\"\"\n",
    "        Initialize the Scraper object.\n",
    "\n",
    "        Args:\n",
    "            text_analysis (TextAnalysis): An instance of the TextAnalysis class used\n",
    "                for text analysis and data storage.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connection = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"scraper\"\n",
    "        );\n",
    "        \n",
    "        self.textanalysis = TextAnalysis()\n",
    "        \n",
    "        # Create a cursor object for executing SQL queries\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def get_data_from_xlsx_to_sql(self,file_name):\n",
    "        \"\"\"\n",
    "        Load data from an Excel file into a SQL database table.\n",
    "    \n",
    "        Before running this method, ensure that you have set up the required database\n",
    "        and tables in your database schema.\n",
    "    \n",
    "        Parameters:\n",
    "            file_name (str): The name of the Excel file containing the data.\n",
    "                The file should be located in the same working directory as this script.\n",
    "    \n",
    "        Raises:\n",
    "            Exception: If an error occurs during the data insertion process, it will\n",
    "                roll back the transaction and print an error message.\n",
    "    \n",
    "        Returns:\n",
    "            None: This method performs the data loading process and does not return a value.\n",
    "        \n",
    "        Example Usage:\n",
    "            To load data from 'Input.xlsx' into the 'INPUTLINKS' table, use:\n",
    "            obj.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "        \"\"\"\n",
    "        \n",
    "        self.file_path = os.path.join(os.getcwd(),file_name)\n",
    "        self.df = pd.read_excel(self.file_path)\n",
    "        list_of_tuples = [tuple(row) for row in self.df.to_records(index=False)]\n",
    "            \n",
    "        \n",
    "        try:           \n",
    "            # Define the INSERT INTO statement\n",
    "            insert_query = \"INSERT INTO INPUTLINKS (URL_ID, URL) VALUES (%s, %s)\"\n",
    "            \n",
    "            # Loop through the list and execute the INSERT INTO statement for each tuple\n",
    "            for values in list_of_tuples:\n",
    "                self.cursor.execute(insert_query, values)\n",
    "            \n",
    "            # Commit the transaction to save the changes to the database\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()\n",
    "            print('Insertion Failed')\n",
    "        else:\n",
    "            print('Insertion Successful')\n",
    "\n",
    "    def partition_raw_data(self,chunk_size,table_name):\n",
    "        '''\n",
    "        Partition a list into smaller equal-sized sublists efficiently.\n",
    "\n",
    "        Parameters:\n",
    "            data_list (list): The list to be partitioned.\n",
    "            chunk_size (int): The size of each partition.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of smaller sublists, each containing 'chunk_size' elements.\n",
    "\n",
    "        usage example:\n",
    "            # Create a generator\n",
    "            result_generator = partition_raw_data(chunk_size=2)\n",
    "\n",
    "            # Iterate through the generator to get the sublists\n",
    "            result_generator = scrap.partition_raw_data(57,'INPUTLINKS')\n",
    "            rawdata_first_half = next(result_generator)\n",
    "            rawdata_second_half = next(result_generator)\n",
    "        '''\n",
    "        \n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.cursor.execute(query)\n",
    "        while True:\n",
    "            rows = self.cursor.fetchmany(chunk_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "\n",
    "    def scrape_data_from_url(self,data):\n",
    "       '''\n",
    "        Prerequisites : Before you run this code make sure you have relevant database and database tables to store.\n",
    "        \n",
    "        Info : Scrape the title and content of articles from a list of URLs and store them\n",
    "        in the database.\n",
    "        \n",
    "        Parameters : \n",
    "              data (List of Tuples) : [\n",
    "                  (123,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/'),\n",
    "                  (124,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-1/'),\n",
    "                  (125,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-0/')\n",
    "              ]\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs scraping and data storage.      \n",
    "       '''\n",
    "        \n",
    "       if not data:\n",
    "           return {'Error':'Please provide valid data type.'}\n",
    "       else:\n",
    "           if isinstance(data,list):\n",
    "               for value in data:\n",
    "                url_id,url = value\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Check if the request was successful (status code 200)\n",
    "                if response.status_code == 200:\n",
    "                    html_content = response.text\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                    title = soup.find('h1', class_='entry-title').text if soup.find('h1', class_='entry-title') is not None else soup.find('h1', class_='tdb-title-text').text if soup.find('h1', class_='tdb-title-text') is not None else None\n",
    "                    \n",
    "                    # Find the <div> with class 'td-post-content'\n",
    "                    post_content_div = soup.find('div', class_='td-post-content') if soup.find('div', class_='td-post-content') is not None else soup.find('div', class_='tdb-block-inner') if soup.find('div', class_='tdb-block-inner') is not None else None\n",
    "                        \n",
    "                    # Check if the <div> was found \n",
    "                    if post_content_div and title is not None: \n",
    "                        # if got the content and title in given url....\n",
    "                           \n",
    "                        # Find all <p> elements inside the <div> without a class name\n",
    "                        paragraphs_without_class =  post_content_div.find_all('p', class_=False)\n",
    "                           \n",
    "                        # Loop through the selected <p> elements and print their text content\n",
    "                        content = str()\n",
    "                        for paragraph in paragraphs_without_class:\n",
    "                            content += paragraph.get_text()\n",
    "                        try:\n",
    "                            with database_lock:\n",
    "                                query = 'INSERT INTO SCRAPERAWDATA(URL_ID,TITLE,CONTENT) VALUES(%s,%s,%s)'\n",
    "                                values = (url_id,title,content)\n",
    "                                self.cursor.execute(query, values)\n",
    "                                self.connection.commit()\n",
    "                        except Exception as e:\n",
    "                            with database_lock:\n",
    "                                query = 'INSERT INTO SCRAPERAWDATA(URL_ID,TITLE,CONTENT) VALUES(%s,%s,%s)'\n",
    "                                values = (url_id,None,None)\n",
    "                                self.cursor.execute(query, values)\n",
    "                                self.connection.commit()\n",
    "                        else:\n",
    "                            print('URL_ID',url_id,'Process Successfully')\n",
    "                else:\n",
    "                    print(f'No Content Found at {url_id}')\n",
    "           else:\n",
    "               return {'Error':'Please provide valid data type.'}\n",
    "               \n",
    "\n",
    "    def store_document_analysis(self,data):\n",
    "        \"\"\"\n",
    "        Use TextAnalysis to perform analysis on text data and store the results\n",
    "        in the database.\n",
    "\n",
    "        Parameters:\n",
    "            text_data (list): A list of text documents for analysis.\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs text analysis and data storage.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not data:\n",
    "            return {'Error':'Please provide valid data type.'}\n",
    "        else:\n",
    "            if isinstance(data,list):\n",
    "                for value in data:\n",
    "                    if value:\n",
    "                        url_id,title,content = value\n",
    "                        content_analysis_detail = self.textanalysis.document_analysis(content)\n",
    "                        try:\n",
    "                            with database_lock:\n",
    "                                query = '''INSERT INTO SCRAPETEXTANALYSIS(URL_ID,POSITIVE_S,NEGATIVE_S,POLARITY_S,\n",
    "                                SUBJECTIV_S,PER_COMPLEX_WORDS,COMPLEX_WORDS,TOTAL_WORD_COUNT,FOG_INDEX,AVG_WORD_PER_SEN,\n",
    "                                SYLLABLE_PER_WORD,AVG_WORD_LENGTH,PERSONAL_PRONOUN) \n",
    "                                VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                                '''\n",
    "                                values = (url_id,\n",
    "                                          content_analysis_detail['positive_score'],\n",
    "                                          content_analysis_detail['negative_score'],\n",
    "                                          content_analysis_detail['polarity_score'],\n",
    "                                          content_analysis_detail['subjectivity_score'],\n",
    "                                          content_analysis_detail['percentage_complex_words'],\n",
    "                                          content_analysis_detail['complex_word_count'],\n",
    "                                          content_analysis_detail['total_word_count'],\n",
    "                                          content_analysis_detail['fog_index'],\n",
    "                                          content_analysis_detail['avg_words_per_sentence'],\n",
    "                                          content_analysis_detail['syllable_per_word'],\n",
    "                                          content_analysis_detail['avg_word_length'],\n",
    "                                          content_analysis_detail['personal_pronoun']\n",
    "                                         )     \n",
    "                                self.cursor.execute(query, values)\n",
    "                                self.connection.commit()\n",
    "                        except Exception as e:\n",
    "                            with database_lock:\n",
    "                                query = '''INSERT INTO SCRAPETEXTANALYSIS(URL_ID,POSITIVE_S,NEGATIVE_S,POLARITY_S,\n",
    "                                SUBJECTIV_S,PER_COMPLEX_WORDS,COMPLEX_WORDS,TOTAL_WORD_COUNT,FOG_INDEX,AVG_WORD_PER_SEN,\n",
    "                                SYLLABLE_PER_WORD,AVG_WORD_LENGTH,PERSONAL_PRONOUN) \n",
    "                                VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                                '''\n",
    "                                values = (url_id,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None,\n",
    "                                          None\n",
    "                                         )     \n",
    "                                self.cursor.execute(query, values)\n",
    "                                self.connection.commit()\n",
    "                        else:\n",
    "                            # to confirm the processed url_id\n",
    "                            print('URL_ID',url_id,'Process Successfully')\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                return {'Error':'Please provide valid data type.'}\n",
    "\n",
    "    def get_document_analysis(self):\n",
    "        \"\"\"\n",
    "                Fetches document analysis data from the database and saves it as an Excel file.\n",
    "            \n",
    "                This method retrieves document analysis information, including sentiment scores, word counts,\n",
    "                and other metrics, for URLs stored in the 'INPUTLINKS' table. It performs an SQL query to join\n",
    "                data from 'INPUTLINKS' and 'SCRAPETEXTANALYSIS' tables, processes the data, and then saves it\n",
    "                as an Excel file for further analysis or reporting.\n",
    "            \n",
    "                Returns:\n",
    "                    None\n",
    "            \n",
    "                Example:\n",
    "                    scraper = Scraper(TextAnalysis)\n",
    "                    scraper.get_document_analysis()\n",
    "        \"\"\"\n",
    "        query = '''SELECT IL.URL_ID, IL.URL,STA.POSITIVE_S,STA.NEGATIVE_S,STA.POLARITY_S,\n",
    "                                STA.SUBJECTIV_S,STA.PER_COMPLEX_WORDS,STA.COMPLEX_WORDS,STA.TOTAL_WORD_COUNT,STA.FOG_INDEX,\n",
    "                                STA.AVG_WORD_PER_SEN,STA.SYLLABLE_PER_WORD,STA.AVG_WORD_LENGTH,STA.PERSONAL_PRONOUN\n",
    "                    FROM INPUTLINKS IL\n",
    "                    INNER JOIN SCRAPETEXTANALYSIS STA ON IL.URL_ID = STA.URL_ID;'''\n",
    "           \n",
    "        \n",
    "        df = pd.read_sql(query, self.connection)\n",
    "        self.connection.close()\n",
    "        # Specify the Excel file path and sheet name\n",
    "        excel_file_path = \"output.xlsx\"\n",
    "        sheet_name = \"Sheet1\"\n",
    "        \n",
    "        # Save the DataFrame to an Excel file\n",
    "        df.to_excel(excel_file_path, sheet_name=sheet_name, index=False, engine='openpyxl')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fcab81-afdf-4d26-bbdc-ac45125616e4",
   "metadata": {},
   "source": [
    "## 5. Code Explanation: Scraping and Data Processing Workflow\r\n",
    "\r\n",
    "In the following code snippet, we demonstrate a workflow that involves creating a `Scraper` instance, loading data from an Excel file into an SQL database, partitioning data, and preparing for web scrapin\n",
    "\n",
    "we demonstrate the use of multithreading to perform web scraping tasks concurrently using two separate threads.g.\r\n",
    "\r\n",
    "### Step 1: Create a Scraper Instance\r\n",
    "```python\r\n",
    "scrap = Scraper(TextAnat_half,))\r\n",
    "ysis)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14f3dc-69c6-4aeb-92f6-f538187c5932",
   "metadata": {},
   "source": [
    "### Step 2: Create Threads\n",
    "```python\n",
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_second_half,))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4bc753-2ce6-45ed-9443-bc3fbf62a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Successful\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraper(TextAnalysis)\n",
    "scrap.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "result_generator = scrap.partition_raw_data(57,'INPUTLINKS')\n",
    "rawdata_first_half = next(result_generator)\n",
    "rawdata_second_half = next(result_generator)\n",
    "# scrap.scrape_data_from_url(rawdata_second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6543be92-c590-41b7-a6f5-be318015bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL_ID 123 Process Successfully\n",
      "URL_ID 26907 Process Successfully\n",
      "URL_ID 27369 Process Successfully\n",
      "URL_ID 321 Process Successfully\n",
      "URL_ID 432 Process Successfully\n",
      "URL_ID 27831 Process Successfully\n",
      "URL_ID 28293 Process Successfully\n",
      "URL_ID 2345 Process Successfully\n",
      "URL_ID 2894 Process Successfully\n",
      "URL_ID 28755 Process Successfully\n",
      "URL_ID 3356 Process Successfully\n",
      "URL_ID 29216 Process Successfully\n",
      "URL_ID 3817 Process Successfully\n",
      "URL_ID 29678 Process Successfully\n",
      "URL_ID 30140 Process Successfully\n",
      "URL_ID 4279 Process Successfully\n",
      "URL_ID 30602 Process Successfully\n",
      "URL_ID 4321 Process Successfully\n",
      "URL_ID 31064 Process Successfully\n",
      "URL_ID 4741 Process Successfully\n",
      "URL_ID 31525 Process Successfully\n",
      "URL_ID 5203 Process Successfully\n",
      "URL_ID 5665 Process Successfully\n",
      "URL_ID 31987 Process Successfully\n",
      "URL_ID 32449 Process Successfully\n",
      "URL_ID 6126 Process Successfully\n",
      "URL_ID 32911 Process Successfully\n",
      "URL_ID 6588 Process Successfully\n",
      "URL_ID 33373 Process Successfully\n",
      "URL_ID 7050 Process Successfully\n",
      "URL_ID 7512 Process Successfully\n",
      "URL_ID 33834 Process Successfully\n",
      "URL_ID 7974 Process Successfully\n",
      "URL_ID 34296 Process Successfully\n",
      "URL_ID 34758 Process Successfully\n",
      "URL_ID 8435 Process Successfully\n",
      "URL_ID 35220 Process Successfully\n",
      "URL_ID 8897 Process Successfully\n",
      "URL_ID 35682 Process Successfully\n",
      "URL_ID 9359 Process Successfully\n",
      "URL_ID 9821 Process Successfully\n",
      "URL_ID 36143 Process Successfully\n",
      "URL_ID 36605 Process Successfully\n",
      "URL_ID 10283 Process Successfully\n",
      "URL_ID 37067 Process Successfully\n",
      "URL_ID 10744 Process Successfully\n",
      "URL_ID 37529 Process Successfully\n",
      "URL_ID 11206 Process Successfully\n",
      "URL_ID 37991 Process Successfully\n",
      "No Content Found at 11668\n",
      "URL_ID 38452 Process Successfully\n",
      "URL_ID 12130 Process Successfully\n",
      "URL_ID 38914 Process Successfully\n",
      "URL_ID 12592 Process Successfully\n",
      "URL_ID 39376 Process Successfully\n",
      "URL_ID 13053 Process Successfully\n",
      "URL_ID 39838 Process Successfully\n",
      "URL_ID 13515 Process Successfully\n",
      "URL_ID 40300 Process Successfully\n",
      "URL_ID 13977 Process Successfully\n",
      "URL_ID 40761 Process Successfully\n",
      "URL_ID 14439 Process Successfully\n",
      "URL_ID 41223 Process Successfully\n",
      "URL_ID 14901 Process Successfully\n",
      "URL_ID 15362 Process Successfully\n",
      "URL_ID 41685 Process Successfully\n",
      "URL_ID 15824 Process Successfully\n",
      "URL_ID 42147 Process Successfully\n",
      "URL_ID 16286 Process Successfully\n",
      "URL_ID 42609 Process Successfully\n",
      "URL_ID 16748 Process Successfully\n",
      "URL_ID 43070 Process Successfully\n",
      "URL_ID 17210 Process Successfully\n",
      "URL_ID 43532 Process Successfully\n",
      "URL_ID 43994 Process Successfully\n",
      "No Content Found at 17671\n",
      "URL_ID 44456 Process Successfully\n",
      "URL_ID 18133 Process Successfully\n",
      "URL_ID 44918 Process Successfully\n",
      "URL_ID 18595 Process Successfully\n",
      "URL_ID 45379 Process Successfully\n",
      "URL_ID 19057 Process Successfully\n",
      "URL_ID 45841 Process Successfully\n",
      "URL_ID 19519 Process Successfully\n",
      "URL_ID 46303 Process Successfully\n",
      "URL_ID 19980 Process Successfully\n",
      "URL_ID 46765 Process Successfully\n",
      "URL_ID 20442 Process Successfully\n",
      "URL_ID 47227 Process Successfully\n",
      "URL_ID 20904 Process Successfully\n",
      "URL_ID 47688 Process Successfully\n",
      "URL_ID 21366 Process Successfully\n",
      "URL_ID 48150 Process Successfully\n",
      "URL_ID 21828 Process Successfully\n",
      "URL_ID 48612 Process Successfully\n",
      "URL_ID 22289 Process Successfully\n",
      "URL_ID 49074 Process Successfully\n",
      "URL_ID 22751 Process Successfully\n",
      "URL_ID 49536 Process Successfully\n",
      "URL_ID 23213 Process Successfully\n",
      "URL_ID 49997 Process Successfully\n",
      "URL_ID 23675 Process Successfully\n",
      "URL_ID 50459 Process Successfully\n",
      "URL_ID 24137 Process Successfully\n",
      "URL_ID 50921 Process Successfully\n",
      "URL_ID 24598 Process Successfully\n",
      "URL_ID 51383 Process Successfully\n",
      "URL_ID 25060 Process Successfully\n",
      "URL_ID 51845 Process Successfully\n",
      "URL_ID 25522 Process Successfully\n",
      "URL_ID 52306 Process Successfully\n",
      "URL_ID 25984 Process Successfully\n",
      "URL_ID 52768 Process Successfully\n",
      "URL_ID 26446 Process Successfully\n"
     ]
    }
   ],
   "source": [
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_second_half,))\n",
    "        \n",
    "# Start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "        \n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee1c4e18-1455-40e7-a554-8e7893cc108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Failed\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraper(TextAnalysis)\n",
    "scrap.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "result_generator = scrap.partition_raw_data(57,'SCRAPERAWDATA')\n",
    "rawdata_first_half = next(result_generator)\n",
    "rawdata_second_half = next(result_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "357336e5-1769-4297-a868-7a0c5ac5d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL_ID 123 Process Successfully\n",
      "URL_ID 27831 Process Successfully\n",
      "URL_ID 321 Process Successfully\n",
      "URL_ID 432 Process Successfully\n",
      "URL_ID 28293 Process Successfully\n",
      "URL_ID 28755 Process Successfully\n",
      "URL_ID 2345 Process Successfully\n",
      "URL_ID 29216 Process Successfully\n",
      "URL_ID 2894 Process Successfully\n",
      "URL_ID 29678 Process Successfully\n",
      "URL_ID 3356 Process Successfully\n",
      "URL_ID 30140 Process Successfully\n",
      "URL_ID 30602 Process Successfully\n",
      "URL_ID 31064 Process Successfully\n",
      "URL_ID 31525 Process Successfully\n",
      "URL_ID 3817 Process Successfully\n",
      "URL_ID 4279 Process Successfully\n",
      "URL_ID 4321 Process Successfully\n",
      "URL_ID 4741 Process Successfully\n",
      "URL_ID 5203 Process Successfully\n",
      "URL_ID 31987 Process Successfully\n",
      "URL_ID 5665 Process Successfully\n",
      "URL_ID 6126 Process Successfully\n",
      "URL_ID 32449 Process Successfully\n",
      "URL_ID 6588 Process Successfully\n",
      "URL_ID 7050 Process Successfully\n",
      "URL_ID 7512 Process Successfully\n",
      "URL_ID 32911 Process Successfully\n",
      "URL_ID 33373 Process Successfully\n",
      "URL_ID 7974 Process Successfully\n",
      "URL_ID 33834 Process Successfully\n",
      "URL_ID 8435 Process Successfully\n",
      "URL_ID 8897 Process Successfully\n",
      "URL_ID 34296 Process Successfully\n",
      "URL_ID 9359 Process Successfully\n",
      "URL_ID 34758 Process Successfully\n",
      "URL_ID 35220 Process Successfully\n",
      "URL_ID 9821 Process Successfully\n",
      "URL_ID 35682 Process Successfully\n",
      "URL_ID 36143 Process Successfully\n",
      "URL_ID 10283 Process Successfully\n",
      "URL_ID 10744 Process Successfully\n",
      "URL_ID 36605 Process Successfully\n",
      "URL_ID 11206 Process Successfully\n",
      "URL_ID 37067 Process Successfully\n",
      "URL_ID 12130 Process Successfully\n",
      "URL_ID 37529 Process Successfully\n",
      "URL_ID 37991 Process Successfully\n",
      "URL_ID 12592 Process Successfully\n",
      "URL_ID 38452 Process Successfully\n",
      "URL_ID 38914 Process Successfully\n",
      "URL_ID 39376 Process Successfully\n",
      "URL_ID 13053 Process Successfully\n",
      "URL_ID 39838 Process Successfully\n",
      "URL_ID 40300 Process Successfully\n",
      "URL_ID 40761 Process Successfully\n",
      "URL_ID 13515 Process Successfully\n",
      "URL_ID 41223 Process Successfully\n",
      "URL_ID 41685 Process Successfully\n",
      "URL_ID 42147 Process Successfully\n",
      "URL_ID 13977 Process Successfully\n",
      "URL_ID 14439 Process Successfully\n",
      "URL_ID 42609 Process Successfully\n",
      "URL_ID 43070 Process Successfully\n",
      "URL_ID 14901 Process Successfully\n",
      "URL_ID 15362 Process Successfully\n",
      "URL_ID 15824 Process Successfully\n",
      "URL_ID 43532 Process Successfully\n",
      "URL_ID 16286 Process Successfully\n",
      "URL_ID 16748 Process Successfully\n",
      "URL_ID 17210 Process Successfully\n",
      "URL_ID 43994 Process Successfully\n",
      "URL_ID 18133 Process Successfully\n",
      "URL_ID 18595 Process Successfully\n",
      "URL_ID 44456 Process Successfully\n",
      "URL_ID 44918 Process Successfully\n",
      "URL_ID 45379 Process Successfully\n",
      "URL_ID 19057 Process Successfully\n",
      "URL_ID 45841 Process Successfully\n",
      "URL_ID 19519 Process Successfully\n",
      "URL_ID 46303 Process Successfully\n",
      "URL_ID 19980 Process Successfully\n",
      "URL_ID 20442 Process Successfully\n",
      "URL_ID 20904 Process Successfully\n",
      "URL_ID 21366 Process Successfully\n",
      "URL_ID 21828 Process Successfully\n",
      "URL_ID 46765 Process Successfully\n",
      "URL_ID 22289 Process Successfully\n",
      "URL_ID 22751 Process Successfully\n",
      "URL_ID 47227 Process Successfully\n",
      "URL_ID 23213 Process Successfully\n",
      "URL_ID 23675 Process Successfully\n",
      "URL_ID 24137 Process Successfully\n",
      "URL_ID 47688 Process Successfully\n",
      "URL_ID 24598 Process Successfully\n",
      "URL_ID 48150 Process Successfully\n",
      "URL_ID 25060 Process Successfully\n",
      "URL_ID 48612 Process Successfully\n",
      "URL_ID 49074 Process Successfully\n",
      "URL_ID 25522 Process Successfully\n",
      "URL_ID 49536 Process Successfully\n",
      "URL_ID 25984 Process Successfully\n",
      "URL_ID 49997 Process Successfully\n",
      "URL_ID 50459 Process Successfully\n",
      "URL_ID 50921 Process Successfully\n",
      "URL_ID 26446 Process Successfully\n",
      "URL_ID 51383 Process Successfully\n",
      "URL_ID 26907 Process Successfully\n",
      "URL_ID 27369 Process Successfully\n",
      "URL_ID 51845 Process Successfully\n",
      "URL_ID 52306 Process Successfully\n",
      "URL_ID 52768 Process Successfully\n"
     ]
    }
   ],
   "source": [
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.store_document_analysis, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.store_document_analysis, args=(rawdata_second_half,))\n",
    "        \n",
    "# Start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "        \n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70a2b8-a0e3-4789-bb4b-14b3f7270810",
   "metadata": {},
   "source": [
    "# 6. Exporting the Text Analysis in Excel Format\r\n",
    "\r\n",
    "In this section, we'll discuss how to export the results of our text analysis in Excel format. This step is essential for making our project more user-friendly and accessible, as it allows users to conveniently analyze and visualize the data.\r\n",
    "\r\n",
    "By following the instructions in this section, you'll learn how to:\r\n",
    "\r\n",
    "- Extract the text analysis results from our project.\r\n",
    "- Create an Excel file to store the data.\r\n",
    "- Organize the data in a structured manner within the Excel file.\r\n",
    "- Utilize Excel's features for further analysis, reporting, or sharing.\r\n",
    "\r\n",
    "Exporting the text analysis results to Excel provides users with a familiar and versatile format for working with the data. Whether it's for generating reports, conducting additional analysis, or sharing insights, Excel format enhances the usability of our project's output.\r\n",
    "\r\n",
    "Let's dive into the details of exporting our text analysis results in Excel format.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a463f14-abe2-402c-9947-8a1ea1e8aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_13068\\3570899897.py:288: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, self.connection)\n"
     ]
    }
   ],
   "source": [
    "scrap.get_document_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
