{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29b21b3d-3aea-423d-8e6f-39056629405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipython-sql\n",
    "!pip install -q pandas mysql-connector-python\n",
    "!pip install -q mysqlclient\n",
    "!pip install -q pymysql\n",
    "!pip install -q openpyxl\n",
    "!pip install -q beautifulsoup4\n",
    "!pip install -q nltk\n",
    "!pip install -q textblob \n",
    "!pip install -q syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98878524-d68b-4a81-8173-6c021fbd3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import syllables\n",
    "import mysql.connector\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb5c7c6-129f-48fd-aa22-936470512092",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b76f32-a863-4138-a2e6-aaab17fd93e9",
   "metadata": {},
   "source": [
    "## Setup Basic Prerequisite: Database Connections and Tables\r\n",
    "\r\n",
    "- In this section, we utilize `ipython-sql` and the `mysql extension` to establish database connections.\r\n",
    "\r\n",
    "- We set up database tables to facilitate data storage and retrieval for analysis purposes.\r\n",
    "\r\n",
    "- The following database tables have been created:\r\n",
    "  - INPUTLINKS\r\n",
    "  - SCRAPERAWDATA\r\n",
    "  - SCRAPETEXTANALYSIS\r\n",
    "\r\n",
    "- These tables are designed to enable seamless data storage, retrieval, and updates as needed.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d8a3a8-3a47-4634-853c-73eca699742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dcca0b-2f9d-4303-9ca9-2cc1982bf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql mysql+mysqlconnector://root:root@localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902ef45f-ce4c-4edc-88a6-45d2df62cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql CREATE DATABASE IF NOT EXISTS scraper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9b008b-d3d1-4af2-b393-8a5f8e4f738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql USE scraper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683bc54d-440e-446e-b23d-8bcdcc30a6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+mysqlconnector://root:***@localhost\n",
      "0 rows affected.\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE TABLE INPUTLINKS(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    URL VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE SCRAPERAWDATA(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    TITLE VARCHAR(255),\n",
    "    CONTENT TEXT,\n",
    "    FOREIGN KEY (URL_ID) REFERENCES INPUTLINKS(URL_ID)\n",
    ");\n",
    "\n",
    "CREATE TABLE SCRAPETEXTANALYSIS(\n",
    "    URL_ID INT PRIMARY KEY,\n",
    "    POSITIVE_S FLOAT,\n",
    "    NEGATIVE_S FLOAT,\n",
    "    POLARITY_S FLOAT,\n",
    "    SUBJECTIV_S FLOAT,\n",
    "    PER_COMPLEX_WORDS FLOAT,\n",
    "    COMPLEX_WORDS INT,\n",
    "    TOTAL_WORD_COUNT INT,\n",
    "    FOG_INDEX FLOAT,\n",
    "    AVG_WORD_PER_SEN FLOAT,\n",
    "    SYLLABLE_PER_WORD FLOAT,\n",
    "    AVG_WORD_LENGTH FLOAT,\n",
    "    PERSONAL_PRONOUN INT,\n",
    "    FOREIGN KEY (URL_ID) REFERENCES SCRAPERAWDATA(URL_ID)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd505c-9a20-42ec-bff6-c7d372f0261b",
   "metadata": {},
   "source": [
    "## TextAnalysis Class: Composition with Scraper Class\r\n",
    "\r\n",
    "- The `TextAnalysis` class leverages text analysis libraries such as `NLTK` to analyze text data and store the results in database tables.\r\n",
    "\r\n",
    "- This class serves the purpose of separating the text analysis process, promoting modular design.\r\n",
    "\r\n",
    "- It establishes a composition relationship with the `Scraper` class, allowing for the integration of text analysis capabilities within the broader scraping and data processing context.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e59fee-c5ec-440d-92b7-df5b3bbd9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalysis:\n",
    "    \"\"\"\n",
    "    The TextAnalysis class provides methods for text preprocessing and analysis.\n",
    "\n",
    "    This class includes methods to preprocess text by removing HTML tags, punctuation,\n",
    "    and stopwords, as well as performing text analysis and calculating various\n",
    "    metrics such as sentiment scores, readability, and word statistics.\n",
    "\n",
    "    Attributes:\n",
    "        None\n",
    "\n",
    "    Methods:\n",
    "        preprocesstext(text):\n",
    "            Preprocess the input text by tokenizing, removing HTML tags, punctuation,\n",
    "            and stopwords, and lemmatizing words.\n",
    "\n",
    "        document_analysis(text):\n",
    "            Calculate various text analysis metrics, including sentiment scores,\n",
    "            subjectivity, average sentence length, percentage of complex words, FOG Index,\n",
    "            average number of words per sentence, complex word count, total word count,\n",
    "            syllables per word, average word length, and personal pronoun count.\n",
    "\n",
    "    Example Usage:\n",
    "        analyzer = TextAnalysis()\n",
    "        preprocessed_text = analyzer.preprocesstext(raw_text)\n",
    "        analysis_results = analyzer.document_analysis(analyzed_text)\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocesstext(self,text):\n",
    "        \"\"\"\n",
    "        Preprocess the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text after removing HTML tags, punctuation,\n",
    "                 stopwords, and lemmatization.\n",
    "        \"\"\"\n",
    "        newtext = str() \n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Define a regex pattern to match HTML tags\n",
    "            html_tags_pattern = r'<[^>]+>'\n",
    "            \n",
    "            # Use re.sub to replace HTML tags with an empty string\n",
    "            sentence = re.sub(html_tags_pattern, '', sentence)\n",
    "        \n",
    "            # Define a regex pattern to match punctuation characters\n",
    "            punctuation_pattern = r'[{}]'.format(re.escape(string.punctuation))\n",
    "                \n",
    "            # Use re.sub to replace punctuation with an empty string\n",
    "            sentence = re.sub(punctuation_pattern, '', sentence)\n",
    "        \n",
    "            # remove stop words\n",
    "            sentence = ''.join([word for word in sentence if word not in stopwords.words('english')])\n",
    "\n",
    "            # Lemmatize words\n",
    "            sentence = ''.join([lemmatizer.lemmatize(word) for word in sentence])\n",
    "    \n",
    "            newtext += sentence\n",
    "    \n",
    "        return newtext\n",
    "\n",
    "    \n",
    "    def __count_complex_words(self,blob):\n",
    "       words = blob.words\n",
    "    \n",
    "       # Count the number of complex words (words with more than two syllables)\n",
    "       complex_word_count = sum(1 for word in words if syllables.estimate(word) > 2)\n",
    "    \n",
    "       # Total word count\n",
    "       total_word_count = len(words)\n",
    "    \n",
    "       return complex_word_count, total_word_count     \n",
    "\n",
    "\n",
    "    def __calculate_percentage_complex_words(self,blob):\n",
    "       complex_word_count, total_word_count = self.__count_complex_words(blob)\n",
    "      \n",
    "       percentage_complex_words = (complex_word_count / total_word_count) * 100\n",
    "\n",
    "       return percentage_complex_words, complex_word_count, total_word_count\n",
    "\n",
    "\n",
    "    def __count_pronouns(self,blob):      \n",
    "      # Get the list of tags for each word in the text\n",
    "      tags = blob.tags\n",
    "      \n",
    "      # Define the set of personal pronoun tags\n",
    "      personal_pronouns = set(['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "      \n",
    "      # Count the number of personal pronouns in the text\n",
    "      pronoun_count = sum(1 for word, tag in tags if tag in personal_pronouns)\n",
    "      \n",
    "      return pronoun_count\n",
    "\n",
    "\n",
    "    def document_analysis(self,text):\n",
    "        '''\n",
    "        Info : Analyze the input text and calculate various text analysis metrics.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing various text analysis metrics,\n",
    "                  such as sentiment scores, subjectivity, average sentence length,\n",
    "                  percentage of complex words, FOG Index, average number of words\n",
    "                  per sentence, complex word count, total word count, syllables\n",
    "                  per word, average word length, and personal pronoun count.\n",
    "                  \n",
    "              Docs : This method calculate the following metrics:\n",
    "              Positive Score: The number of positive words or sentiments expressed in the text.\n",
    "              Negative Score: The number of negative words or sentiments expressed in the text.\n",
    "              Polarity Score: The polarity score indicates the overall sentiment of the text. It can be calculated as (Positive Score - Negative Score).\n",
    "              Subjectivity Score: The subjectivity score measures the degree of subjectivity or objectivity in the text. It usually ranges from 0 to 1, where 0 represents an objective text and 1 represents a highly subjective text.\n",
    "              Avg Sentence Length: The average number of words in each sentence of the text.\n",
    "              Percentage of Complex Words: The percentage of words in the text that are considered complex, often measured based on the number of syllables per word or other linguistic complexity measures.\n",
    "              FOG Index: The FOG Index is a readability formula that estimates the years of formal education required to understand the text. It considers sentence length and the percentage of complex words.\n",
    "              Avg Number of Words per Sentence: The average number of words in each sentence of the text.\n",
    "              Complex Word Count: The total count of complex words in the text.\n",
    "              Word Count: The total number of words in the text.\n",
    "              Syllables per Word: The average number of syllables per word in the text.\n",
    "              Personal Pronouns: The count of personal pronouns (e.g., I, you, he, she, we, they) used in the text.\n",
    "              Avg Word Length: The average length of words in the text, typically measured in characters.\n",
    "    '''\n",
    "        text_analysis_details = dict()\n",
    "        # Create a TextBlob object\n",
    "        blob = TextBlob(text)\n",
    "\n",
    "        # Calculate sentiment scores\n",
    "        text_analysis_details['positive_score'] = len([sentence for sentence in blob.sentences if sentence.sentiment.polarity > 0])\n",
    "        text_analysis_details['negative_score'] = len([sentence for sentence in blob.sentences if sentence.sentiment.polarity < 0])\n",
    "        text_analysis_details['polarity_score'] = blob.sentiment.polarity\n",
    "        text_analysis_details['subjectivity_score'] = blob.sentiment.subjectivity\n",
    "     \n",
    "        # Calculate average sentence length\n",
    "        text_analysis_details['avg_sentence_length'] = sum(len(sentence.words) for sentence in blob.sentences) / len(blob.sentences)  \n",
    "      \n",
    "        percentage_complex_words, complex_word_count, total_word_count = self.__calculate_percentage_complex_words(blob)\n",
    "        text_analysis_details['percentage_complex_words'] = percentage_complex_words\n",
    "        text_analysis_details['complex_word_count'] = complex_word_count\n",
    "        text_analysis_details['total_word_count'] = total_word_count\n",
    "     \n",
    "        # Calculate the FOG Index (requires more detailed calculations)\n",
    "        # FOG Index formula: 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        text_analysis_details['fog_index'] = 0.4 * ((sum(len(sentence.words) for sentence in blob.sentences) / len(blob.sentences)) + text_analysis_details['percentage_complex_words'])  \n",
    "     \n",
    "        # Calculate average number of words per sentence\n",
    "        text_analysis_details['avg_words_per_sentence'] = len(blob.words) / len(blob.sentences) \n",
    "     \n",
    "        # Calculate syllabus per word length\n",
    "        text_analysis_details['syllable_per_word']= (sum([syllables.estimate(word) for word in blob.words])/len([syllables.estimate(word) for word in blob.words]))\n",
    "     \n",
    "        # Calculate average word length\n",
    "        text_analysis_details['avg_word_length'] = (sum(len(word) for word in blob.words) / len(blob.words)) \n",
    "\n",
    "        # Calculate personal pronoun\n",
    "        text_analysis_details['personal_pronoun'] = self.__count_pronouns(blob)\n",
    "\n",
    "        return text_analysis_details\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50247d17-487c-42c8-b6b5-e1f53d439087",
   "metadata": {},
   "source": [
    "## Scraper Class: Data Scraping from InputLinks\r\n",
    "\r\n",
    "- The `Scraper` class is designed to efficiently scrape data from the `InputLinks` source using various technologies and techniques.\r\n",
    "\r\n",
    "- Key Technologies:\r\n",
    "  - **MySQL Extension:** It utilizes the `mysql extension` to manage database connections and operations.\r\n",
    "  - **Beautiful Soup 4 (beautifulsoup4):** The class leverages the `beautifulsoup4` library to parse and extract data from web pages with ease.\r\n",
    "  - **Multithreading:** To enhance data retrieval speed, the class employs multithreading, allowing for the concurrent processing of multiple web pages.\r\n",
    "\r\n",
    "- Data Storage:\r\n",
    "  - The scraped data is intended to be stored in a dedicated table named `RAWDATATABLE` within the database for further analysis and processing.\r\n",
    "\r\n",
    "- Purpose:\r\n",
    "  - The primary goal of this class is to streamline the data scraping process, ensuring efficient extraction and storage of data from the `InputLinks` source.\r\n",
    "\r\n",
    "- Note:\r\n",
    "  - Before using this class, ensure that you have set up the necessary database connections and tables to accommodate the scraped data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def0c560-dd53-43d5-8b9c-075e24de9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \"\"\"\n",
    "        The Scraper class is responsible for web scraping, data processing, and storage.\n",
    "        \n",
    "        It connects to a MySQL database, loads data from an Excel file, partitions data\n",
    "        into smaller sublists, scrapes article data from URLs, performs text analysis, and\n",
    "        stores the results in the database.\n",
    "    \n",
    "        Args:\n",
    "            text_analysis (TextAnalysis): An instance of the TextAnalysis class used for\n",
    "                text analysis and data storage.\n",
    "    \n",
    "        Attributes:\n",
    "            connection (MySQLConnection): A MySQL database connection.\n",
    "            cursor (MySQLCursor): A MySQL cursor for executing queries.\n",
    "            text_analysis (TextAnalysis): An instance of the TextAnalysis class.\n",
    "    \n",
    "        Methods:\n",
    "            __init__(self, text_analysis): Constructor for initializing the Scraper object.\n",
    "            get_data_from_xlsx_to_sql(self, file_name): Load data from an Excel file into a\n",
    "                SQL database table.\n",
    "            partition_raw_data(self, data_list, chunk_size): Partition a list into smaller\n",
    "                equal-sized sublists efficiently.\n",
    "            scrape_data_from_url(self, url_list): Scrape the title and content of articles\n",
    "                from a list of URLs and store them in the database.\n",
    "            store_document_analysis(self, text_data): Use TextAnalysis to perform analysis\n",
    "                on text data and store the results in the database.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,TextAnalysis):\n",
    "        \"\"\"\n",
    "        Initialize the Scraper object.\n",
    "\n",
    "        Args:\n",
    "            text_analysis (TextAnalysis): An instance of the TextAnalysis class used\n",
    "                for text analysis and data storage.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connection = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"scraper\"\n",
    "        );\n",
    "        \n",
    "        self.textanalysis = TextAnalysis()\n",
    "        \n",
    "        # Create a cursor object for executing SQL queries\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def get_data_from_xlsx_to_sql(self,file_name):\n",
    "        \"\"\"\n",
    "        Load data from an Excel file into a SQL database table.\n",
    "    \n",
    "        Before running this method, ensure that you have set up the required database\n",
    "        and tables in your database schema.\n",
    "    \n",
    "        Parameters:\n",
    "            file_name (str): The name of the Excel file containing the data.\n",
    "                The file should be located in the same working directory as this script.\n",
    "    \n",
    "        Raises:\n",
    "            Exception: If an error occurs during the data insertion process, it will\n",
    "                roll back the transaction and print an error message.\n",
    "    \n",
    "        Returns:\n",
    "            None: This method performs the data loading process and does not return a value.\n",
    "        \n",
    "        Example Usage:\n",
    "            To load data from 'Input.xlsx' into the 'INPUTLINKS' table, use:\n",
    "            obj.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "        \"\"\"\n",
    "        \n",
    "        self.file_path = os.path.join(os.getcwd(),file_name)\n",
    "        self.df = pd.read_excel(self.file_path)\n",
    "        list_of_tuples = [tuple(row) for row in self.df.to_records(index=False)]\n",
    "            \n",
    "        \n",
    "        try:           \n",
    "            # Define the INSERT INTO statement\n",
    "            insert_query = \"INSERT INTO INPUTLINKS (URL_ID, URL) VALUES (%s, %s)\"\n",
    "            \n",
    "            # Loop through the list and execute the INSERT INTO statement for each tuple\n",
    "            for values in list_of_tuples:\n",
    "                self.cursor.execute(insert_query, values)\n",
    "            \n",
    "            # Commit the transaction to save the changes to the database\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()\n",
    "            print('Insertion Failed')\n",
    "        else:\n",
    "            print('Insertion Successful')\n",
    "\n",
    "    def partition_raw_data(self,chunk_size,table_name):\n",
    "        '''\n",
    "        Partition a list into smaller equal-sized sublists efficiently.\n",
    "\n",
    "        Parameters:\n",
    "            data_list (list): The list to be partitioned.\n",
    "            chunk_size (int): The size of each partition.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of smaller sublists, each containing 'chunk_size' elements.\n",
    "\n",
    "        usage example:\n",
    "            # Create a generator\n",
    "            result_generator = partition_raw_data(chunk_size=2)\n",
    "\n",
    "            # Iterate through the generator to get the sublists\n",
    "            result_generator = scrap.partition_raw_data(57,'INPUTLINKS')\n",
    "            rawdata_first_half = next(result_generator)\n",
    "            rawdata_second_half = next(result_generator)\n",
    "        '''\n",
    "        \n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.cursor.execute(query)\n",
    "        while True:\n",
    "            rows = self.cursor.fetchmany(chunk_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "\n",
    "    def scrape_data_from_url(self,data):\n",
    "       '''\n",
    "        Prerequisites : Before you run this code make sure you have relevant database and database tables to store.\n",
    "        \n",
    "        Info : Scrape the title and content of articles from a list of URLs and store them\n",
    "        in the database.\n",
    "        \n",
    "        Parameters : \n",
    "              data (List of Tuples) : [\n",
    "                  (123,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/'),\n",
    "                  (124,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-1/'),\n",
    "                  (125,'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-0/')\n",
    "              ]\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs scraping and data storage.      \n",
    "       '''\n",
    "        \n",
    "       if not data:\n",
    "           return {'Error':'Please provide valid data type.'}\n",
    "       else:\n",
    "           if isinstance(data,list):\n",
    "               for value in data:\n",
    "                   url_id,url = value\n",
    "                   response = requests.get(url)\n",
    "\n",
    "                   # Check if the request was successful (status code 200)\n",
    "                   if response.status_code == 200:\n",
    "                       html_content = response.text\n",
    "                       soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                       title = soup.find('h1', class_='entry-title').text if soup.find('h1', class_='entry-title') is not None else soup.find('h1', class_='tdb-title-text').text if soup.find('h1', class_='tdb-title-text') is not None else None\n",
    "                       # Find the <div> with class 'td-post-content'\n",
    "                       post_content_div = soup.find('div', class_='td-post-content') if soup.find('div', class_='td-post-content') is not None else soup.find('div', class_='tdb-block-inner') if soup.find('div', class_='tdb-block-inner') is not None else None\n",
    "                        \n",
    "                       # Check if the <div> was found \n",
    "                       if post_content_div and title is not None: \n",
    "                           # if got the content and title in given url....\n",
    "                           \n",
    "                           # Find all <p> elements inside the <div> without a class name\n",
    "                           paragraphs_without_class =  post_content_div.find_all('p', class_=False)\n",
    "                           \n",
    "                           # Loop through the selected <p> elements and print their text content\n",
    "                           content = str()\n",
    "                           for paragraph in paragraphs_without_class:\n",
    "                               content += paragraph.get_text()\n",
    "                               try:\n",
    "                                   with database_lock:\n",
    "                                       query = 'INSERT INTO SCRAPERAWDATA(URL_ID,TITLE,CONTENT) VALUES(%s,%s,%s)'\n",
    "                                       values = (url_id,title,content)\n",
    "                                       self.cursor.execute(query, values)\n",
    "                                       self.connection.commit()\n",
    "                               except Exception as e:\n",
    "                                   self.connection.rollback()\n",
    "                               else:\n",
    "                                   print('URL_ID',url_id,'Process Successfully')\n",
    "                               finally:\n",
    "                                   print('Scraping and processing text is completed.')\n",
    "                       else:\n",
    "                           query = 'INSERT INTO SCRAPERAWDATA(URL_ID,TITLE,CONTENT) VALUES(%s,%s,%s)'\n",
    "                           values = (url_id,None,None)\n",
    "                           self.cursor.execute(query, values)\n",
    "                           self.connection.commit()\n",
    "                           print(f'No Content Found at {url_id}')\n",
    "                   else:\n",
    "                       return {'Error','Failed to retrieve the web page.'}\n",
    "           else:\n",
    "               return {'Error':'Please provide valid data type.'}\n",
    "\n",
    "    def store_document_analysis(self,data):\n",
    "        \"\"\"\n",
    "        Use TextAnalysis to perform analysis on text data and store the results\n",
    "        in the database.\n",
    "\n",
    "        Parameters:\n",
    "            text_data (list): A list of text documents for analysis.\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs text analysis and data storage.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not data:\n",
    "            return {'Error':'Please provide valid data type.'}\n",
    "        else:\n",
    "            if isinstance(data,list):\n",
    "                for value in data:\n",
    "                    url_id,title,content = value\n",
    "                    try:\n",
    "                        content_analysis_detail = self.textanalysis.document_analysis(content)\n",
    "                        with database_lock:\n",
    "                            query = '''INSERT INTO SCRAPERAWDATA(URL_ID,POSITIVE_S,NEGATIVE_S,POLARITY_S,\n",
    "                            SUBJECTIV_S,PER_COMPLEX_WORDS,COMPLEX_WORDS,TOTAL_WORD_COUNT,FOG_INDEX,AVG_WORD_PER_SEN,\n",
    "                            SYLLABLE_PER_WORD,AVG_WORD_LENGTH,PERSONAL_PRONOUN) \n",
    "                            VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                            '''\n",
    "                            values = (url_id,\n",
    "                                      content_analysis_detail['positive_score'],\n",
    "                                      content_analysis_detail['negative_score'],\n",
    "                                      content_analysis_detail['polarity_score'],\n",
    "                                      content_analysis_detail['subjectivity_score'],\n",
    "                                      content_analysis_detail['avg_sentence_length'],\n",
    "                                      content_analysis_detail['percentage_complex_words'],\n",
    "                                      content_analysis_detail['complex_word_count'],\n",
    "                                      content_analysis_detail['total_word_count''fog_index'],\n",
    "                                      content_analysis_detail['avg_words_per_sentence'],\n",
    "                                      content_analysis_detail['syllable_per_word'],\n",
    "                                      content_analysis_detail['avg_word_length'],\n",
    "                                      content_analysis_detail['personal_pronoun']\n",
    "                                     )     \n",
    "                            self.cursor.execute(query, values)\n",
    "                            self.connection.commit()\n",
    "                    except Exception as e:\n",
    "                        with database_lock:\n",
    "                            query = '''INSERT INTO SCRAPERAWDATA(URL_ID,POSITIVE_S,NEGATIVE_S,POLARITY_S,\n",
    "                            SUBJECTIV_S,PER_COMPLEX_WORDS,COMPLEX_WORDS,TOTAL_WORD_COUNT,FOG_INDEX,AVG_WORD_PER_SEN,\n",
    "                            SYLLABLE_PER_WORD,AVG_WORD_LENGTH,PERSONAL_PRONOUN) \n",
    "                            VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                            '''\n",
    "                            values = (url_id,None,None,None,None,None,None,None,None,None,None,None,None)\n",
    "                            self.cursor.execute(query, values)\n",
    "                            self.connection.commit()\n",
    "                    else:\n",
    "                        print('URL_ID',url_id,'Process Successfully')\n",
    "                    finally:\n",
    "                        print('text analysis is completed.')\n",
    "                    print(url_id,content_analysis_detail)\n",
    "            else:\n",
    "                return {'Error':'Please provide valid data type.'}\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fcab81-afdf-4d26-bbdc-ac45125616e4",
   "metadata": {},
   "source": [
    "## Code Explanation: Scraping and Data Processing Workflow\r\n",
    "\r\n",
    "In the following code snippet, we demonstrate a workflow that involves creating a `Scraper` instance, loading data from an Excel file into an SQL database, partitioning data, and preparing for web scrapin\n",
    "\n",
    "we demonstrate the use of multithreading to perform web scraping tasks concurrently using two separate threads.g.\r\n",
    "\r\n",
    "### Step 1: Create a Scraper Instance\r\n",
    "```python\r\n",
    "scrap = Scraper(TextAna```ts.\r\n",
    "\r\n",
    "### 2tep 1: Create Threads\r\n",
    "```python\r\n",
    "# Create two threads to perform transactions\r\n",
    "thread1 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_first_half,))\r\n",
    "thread2 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_secon\n",
    "```d_half,))\r\n",
    "ysis)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f4bc753-2ce6-45ed-9443-bc3fbf62a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Failed\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraper(TextAnalysis)\n",
    "scrap.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "result_generator = scrap.partition_raw_data(57,'INPUTLINKS')\n",
    "rawdata_first_half = next(result_generator)\n",
    "rawdata_second_half = next(result_generator)\n",
    "# scrap.scrape_data_from_url(rawdata_second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543be92-c590-41b7-a6f5-be318015bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-68 (scrape_data_from_url):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 633, in cmd_query\n",
      "    self._cmysql.query(\n",
      "_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_13660\\3406682376.py\", line 128, in scrape_data_from_url\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py\", line 330, in execute\n",
      "    result = self._cnx.cmd_query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py\", line 77, in wrapper\n",
      "    return method(cnx, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 641, in cmd_query\n",
      "    raise get_mysql_exception(\n",
      "mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_13660\\3406682376.py\", line 131, in scrape_data_from_url\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 517, in rollback\n",
      "    self._cmysql.rollback()\n",
      "_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and processing text is completed.\n",
      "Scraping and processing text is completed.\n",
      "Scraping and processing text is completed.\n",
      "Scraping and processing text is completed.\n"
     ]
    }
   ],
   "source": [
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.scrape_data_from_url, args=(rawdata_second_half,))\n",
    "        \n",
    "# Start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "        \n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c4e18-1455-40e7-a554-8e7893cc108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Failed\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraper(TextAnalysis)\n",
    "scrap.get_data_from_xlsx_to_sql('Input.xlsx')\n",
    "result_generator = scrap.partition_raw_data(57,'SCRAPERAWDATA')\n",
    "rawdata_first_half = next(result_generator)\n",
    "rawdata_second_half = next(result_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357336e5-1769-4297-a868-7a0c5ac5d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (store_document_analysis):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 170, in store_document_analysis\n",
      "Exception in thread Thread-7 (store_document_analysis):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 170, in store_document_analysis\n",
      "KeyError: 'total_word_countfog_index'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 633, in cmd_query\n",
      "KeyError: 'total_word_countfog_index'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 633, in cmd_query\n",
      "    self._cmysql.query(\n",
      "_mysql_connector.MySQLInterfaceError: Unknown column 'POSITIVE_S' in 'field list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self._cmysql.query(\n",
      "_mysql_connector.MySQLInterfaceError: Unknown column 'POSITIVE_S' in 'field list'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 186, in store_document_analysis\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_17224\\3406682376.py\", line 186, in store_document_analysis\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py\", line 330, in execute\n",
      "    result = self._cnx.cmd_query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py\", line 77, in wrapper\n",
      "    return method(cnx, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 641, in cmd_query\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py\", line 330, in execute\n",
      "    raise get_mysql_exception(\n",
      "mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'POSITIVE_S' in 'field list'\n",
      "    result = self._cnx.cmd_query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\opentelemetry\\context_propagation.py\", line 77, in wrapper\n",
      "    return method(cnx, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\acer\\Downloads\\MySQL_with_python\\WebScrapper_Sentiment\\scrapenv\\Lib\\site-packages\\mysql\\connector\\connection_cext.py\", line 641, in cmd_query\n",
      "    raise get_mysql_exception(\n",
      "mysql.connector.errors.ProgrammingError: 1054 (42S22): Unknown column 'POSITIVE_S' in 'field list'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text analysis is completed.\n",
      "text analysis is completed.\n"
     ]
    }
   ],
   "source": [
    "# Create two threads to perform transactions\n",
    "thread1 = threading.Thread(target=scrap.store_document_analysis, args=(rawdata_first_half,))\n",
    "thread2 = threading.Thread(target=scrap.store_document_analysis, args=(rawdata_second_half,))\n",
    "        \n",
    "# Start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "        \n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
